{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax Regression\n",
    "Reference\n",
    "- [MNIST for ML Beginners](https://www.tensorflow.org/get_started/mnist/beginners)\n",
    "- [Deep MNIST for Experts](https://www.tensorflow.org/get_started/mnist/pros)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load MNIST Data\n",
    "\n",
    "The MNIST data is split into three parts: \n",
    "- 55,000 data points of training data (mnist.train), \n",
    "- 10,000 points of test data (mnist.test),  \n",
    "- 5,000 points of validation data (mnist.validation).  \n",
    "Every MNIST data point has two parts: an image of a handwritten digit and a corresponding label.  \n",
    "Each image is 28 pixels by 28 pixels. We can flatten this array into a vector of 28x28 = 784 numbers.  \n",
    "Each image in MNIST has a corresponding label, a number between 0 and 9 representing the digit drawn in the image.  \n",
    "We want our labels as \"**one-hot vectors**\". In this case, the nth digit will be represented as a vector which is 1 in the nth dimension. For example, 3 would be [0,0,0,1,0,0,0,0,0,0].\n",
    "\n",
    "```mnist``` is a lightweight class which stores the training, validation, and testing sets as NumPy arrays. It also provides a function for iterating through data minibatches.\n",
    "\n",
    "Download and read the MNIST data automatically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start TensorFlow InteractiveSession\n",
    "TensorFlow relies on a highly efficient **C++ backend** to do its computation. The connection to this backend is called a session. The common usage for TensorFlow programs is to first create a graph and then launch it in a session.  \n",
    "```InteractiveSession``` class allows you to interleave operations which build a computation graph with ones that run the graph. This is particularly convenient when working in interactive contexts like IPython. If you are not using an InteractiveSession, then you should build the entire computation graph before starting a session and launching the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Softmax Regressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The evidence for a class $k$ given an input $x$ is:\n",
    "$$\\text{evidence}_k (x) = \\sum_j W_{k,j}x_j+b_k$$\n",
    "where $W_k$ is the weights and b_k is the bias for class $k$, and $j$ is an index for summing over the pixels in our input image $x$.  \n",
    "Convert the evidence tallies into the predicted probabilities using the \"softmax\" function:\n",
    "$$p(y=k|x)=\\text{softmax}(\\text{evidence}_k(x))$$\n",
    "Softmax equation:\n",
    "$$\\text{softmax}(\\text{evidence}_k(x))=\\frac{\\text{exp}(\\text{evidence}_k(x))}{\\sum_k\\text{exp}(\\text{evidence}_k(x))}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fox instance, input is a three dimensional array $x = [x_1, x_2, x_3]$ and output is a ternary of softmax values $y = [y_1, y_2, y_3]$.  \n",
    "Picture softmax regression:\n",
    "<img src=\"./MNIST-ML/softmax-regression-picture.png\">\n",
    "Write out as equations:\n",
    "<img src=\"./MNIST-ML/softmax-regression-equation.png\">\n",
    "Matrix multiplication:\n",
    "<img src=\"./MNIST-ML/softmax-regression-matrix.png\">\n",
    "Write compactly:\n",
    "$$y=\\text{softmax}(Wx+b)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement the Regression\n",
    "TensorFlow lets us describe a graph of interacting operations that run entirely outside Python. (Approaches like this can be seen in a few machine learning libraries.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, [None, 784])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$x$ is a **placeholder**, a value that we'll input when we ask TensorFlow to run a computation. We represent $x$ as a 2-D tensor of floating-point numbers, with a shape ```[None, 784]```. (Here ```None``` means that a dimension can be of any length.)  \n",
    "A **variable** is a modifiable tensor that lives in TensorFlow's graph of interacting operations. Before variables can be used within a session, they must be initialized using that session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W = tf.Variable(tf.zeros([784, 10]))\n",
    "b = tf.Variable(tf.zeros([10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = tf.nn.softmax(tf.matmul(x, W) + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we multiply $x$ by $W$ with the expression ```tf.matmul(x, W)```.  We then add $b$, and finally apply ```tf.nn.softmax```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "Cross entropy (expected log-loss) of two distributions $q, p$ where p is the real distribution and q is an estimator:\n",
    "$$H_p(q) = -\\sum_i p_i\\log(q_i).$$\n",
    "As for conditional distribution p(y|x),\n",
    "$$H_p(q) = -\\sum_x \\sum_y p(x)p(y|x)\\log q(y|x).$$\n",
    "In practice, $p(x)$ and $p(y|x)$ both are unkown and some training data ${(x_i,y_i)}_{i=1}^n$ is given, the empirical expected log-loss for minimizing is:\n",
    "$$-\\frac{1}{n}\\sum_{i=1}^n\\log q(y_i|x_i).$$\n",
    "If we denote the true label as a one-hot vector $y\\__i$ and let $y_i$ be the predicted distribution (a vector of softmax values) for $x_i$.\n",
    "Then the empirical expected log-loss for computation benefit is:\n",
    "$$-\\frac{1}{n}\\sum_{i=1}^n \\sum_k y\\__{ik}\\log y_{ik}.$$\n",
    "Note that tensorFlow official document mentions $y\\__i$ is the true distribution is not appropriate. We never know the real distribution and $y\\__i$ is just an outcome of the real distribution conditioned on $x_i$.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_ = tf.placeholder(tf.float32, [None, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The raw formulation of cross-entropy:  \n",
    "\n",
    "```cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))```  \n",
    "\n",
    "First, ```tf.log``` computes the logarithm of each element of $y$. Next, we multiply each element of $y\\_$ with the corresponding element of ```tf.log(y)```. Then ```tf.reduce_sum``` adds the elements in the second dimension of $y$, due to the ```reduction_indices=[1]``` parameter. Finally, ```tf.reduce_mean``` computes the mean over all the examples in the batch.\n",
    "Unfortunatelly, this can be numerically unstable.  \n",
    "Instead, we apply ```tf.nn.softmax_cross_entropy_with_logits``` on the unnormalized logits (e.g., we call ```softmax_cross_entropy_with_logits on tf.matmul(x, W) + b)```, because this more numerically stable function internally computes the softmax activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow knows the entire graph of your computations, it can automatically use the <a href=\"http://colah.github.io/posts/2015-08-Backprop/\">backpropagation algorithm</a> to efficiently determine how your variables affect the loss you ask it to minimize. Then it can apply your choice of optimization algorithm to modify the variables and reduce the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we ask TensorFlow to minimize ```cross_entropy``` using the **gradient descent algorithm** with a learning rate of 0.5. Gradient descent is a simple procedure, where TensorFlow simply shifts each variable a little bit in the direction that reduces the cost. But TensorFlow also provides many other <a href=\"https://www.tensorflow.org/api_guides/python/train#Optimizers\">optimization algorithms</a>: using one is as simple as tweaking one line. \n",
    "\n",
    "What TensorFlow actually does here, behind the scenes, is to add new operations to your graph which implement backpropagation and gradient descent. Then it gives you back a single operation which, when run, does a step of gradient descent training, slightly tweaking your variables to reduce the loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the variables before they can be used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.global_variables_initializer().run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the training step 1000 times. Each step of the loop, we get a \"batch\" of one hundred random data points from our training set. We run ```train_step``` feeding in the batches data to replace the ```placeholder```'s. \n",
    "\n",
    "Using ```feed_dict``` to replace the ```placeholder``` tensors $x$ and $y\\_$ with the training examples. Note that you can replace any tensor in your computation graph using feed_dict -- it's not restricted to just placeholders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for _ in range(1000):\n",
    "  batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "  sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Model\n",
    "```tf.argmax``` gives you the index of the highest entry in a tensor along some axis. Use ```tf.equal``` to check if our prediction matches the truth:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That gives us a list of booleans. To determine what fraction are correct, we cast to floating point numbers and then take the mean. For example, ```[True, False, True, True]``` would become ```[1,0,1,1]``` which would become $0.75$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy on test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9069\n"
     ]
    }
   ],
   "source": [
    "# print(accuracy.eval(feed_dict={x: mnist.test.images, y_: mnist.test.labels}))\n",
    "print(sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels}))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
