{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build models\n",
    "Make a feedforward neural network also called a multi-layer perception."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load MNIST data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define multi-layer perception function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def build_mlp(\n",
    "        input_placeholder, \n",
    "        output_size,\n",
    "        scope, \n",
    "        n_layers=2, \n",
    "        size=64, \n",
    "        activation=tf.tanh,\n",
    "        output_activation=None\n",
    "        ):\n",
    "\n",
    "    if n_layers < 1 or size < 1:\n",
    "        return input_placeholder\n",
    "    with tf.variable_scope(scope):\n",
    "        inputs = input_placeholder\n",
    "        for _ in range(n_layers - 1):\n",
    "            # by default, use_bias = True\n",
    "            inputs = tf.layers.dense(inputs, units = size, activation = activation)\n",
    "        return tf.layers.dense(inputs, units = output_size, activation = output_activation)              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 10)\n",
      "step 0, training accuracy 0.12\n",
      "step 100, training accuracy 0.48\n",
      "step 200, training accuracy 0.6\n",
      "step 300, training accuracy 0.84\n",
      "step 400, training accuracy 0.84\n",
      "step 500, training accuracy 0.88\n",
      "step 600, training accuracy 0.82\n",
      "step 700, training accuracy 0.9\n",
      "step 800, training accuracy 0.88\n",
      "step 900, training accuracy 0.88\n",
      "step 1000, training accuracy 0.96\n",
      "step 1100, training accuracy 0.86\n",
      "step 1200, training accuracy 0.92\n",
      "step 1300, training accuracy 0.9\n",
      "step 1400, training accuracy 0.88\n",
      "step 1500, training accuracy 0.88\n",
      "step 1600, training accuracy 0.9\n",
      "step 1700, training accuracy 0.94\n",
      "step 1800, training accuracy 0.94\n",
      "step 1900, training accuracy 0.9\n",
      "test accuracy 0.908\n"
     ]
    }
   ],
   "source": [
    "# make sure to clear previous computational graph\n",
    "tf.reset_default_graph()\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "y = build_mlp(x, 10, 'multi-layer1', 2, 64)\n",
    "\n",
    "print(y.shape)\n",
    "\n",
    "y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "cross_entropy = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y))\n",
    "\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "  sess.run(tf.global_variables_initializer())\n",
    "  for i in range(2000):\n",
    "    batch = mnist.train.next_batch(50)\n",
    "    if i % 100 == 0:\n",
    "      train_accuracy = accuracy.eval(feed_dict={\n",
    "          x: batch[0], y_: batch[1]})\n",
    "      print('step %d, training accuracy %g' % (i, train_accuracy))\n",
    "    train_step.run(feed_dict={x: batch[0], y_: batch[1]})\n",
    "\n",
    "  print('test accuracy %g' % accuracy.eval(feed_dict={\n",
    "      x: mnist.test.images, y_: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
