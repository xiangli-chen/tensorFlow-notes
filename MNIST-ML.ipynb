{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST for ML Beginners"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The MNIST Data\n",
    "Download and read the MNIST data automatically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MNIST data is split into three parts: 55,000 data points of training data (mnist.train), 10,000 points of test data (mnist.test), and 5,000 points of validation data (mnist.validation).  \n",
    "Every MNIST data point has two parts: an image of a handwritten digit and a corresponding label.  \n",
    "Each image is 28 pixels by 28 pixels. We can flatten this array into a vector of 28x28 = 784 numbers.  \n",
    "Each image in MNIST has a corresponding label, a number between 0 and 9 representing the digit drawn in the image.  \n",
    "We're going to want our labels as \"one-hot vectors\". In this case, the nth digit will be represented as a vector which is 1 in the nth dimension. For example, 3 would be [0,0,0,1,0,0,0,0,0,0]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Regressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The evidence for a class $i$ given an input $x$ is:\n",
    "$$\\text{evidence}_i = \\sum_j W_{i,j}x_j+b_i$$\n",
    "where $W_i$ is the weights and bi is the bias for class $i$, and $j$ is an index for summing over the pixels in our input image $x$.  \n",
    "Convert the evidence tallies into the predicted probabilities $y$ using the \"softmax\" function:\n",
    "$$y=\\text{softmax(evidence)}$$\n",
    "Softmax equation:\n",
    "$$\\text{softmax}(x)_i=\\frac{\\text{exp}(x_i)}{\\sum_j\\text{exp}(x_j)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fox instance, input is a three dimensional array $x = [x_1, x_2, x_3]$ and output is a ternary class $y = [y_1, y_2, y_3]$.  \n",
    "Picture softmax regression:\n",
    "<img src=\"./MNIST-ML/softmax-regression-picture.png\">\n",
    "Write out as equations:\n",
    "<img src=\"./MNIST-ML/softmax-regression-equation.png\">\n",
    "Matrix multiplication:\n",
    "<img src=\"./MNIST-ML/softmax-regression-matrix.png\">\n",
    "Write compactly:\n",
    "$$y=\\text{softmax}(Wx+b)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement the Regression\n",
    "TensorFlow lets us describe a graph of interacting operations that run entirely outside Python. (Approaches like this can be seen in a few machine learning libraries.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "x = tf.placeholder(tf.float32, [None, 784])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$x$ is a **placeholder**, a value that we'll input when we ask TensorFlow to run a computation. We represent $x$ as a 2-D tensor of floating-point numbers, with a shape ```[None, 784]```. (Here ```None``` means that a dimension can be of any length.)  \n",
    "A **Variable** is a modifiable tensor that lives in TensorFlow's graph of interacting operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W = tf.Variable(tf.zeros([784, 10]))\n",
    "b = tf.Variable(tf.zeros([10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = tf.nn.softmax(tf.matmul(x, W) + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we multiply $x$ by $W$ with the expression ```tf.matmul(x, W)```.  We then add $b$, and finally apply ```tf.nn.softmax```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "Cross entropy (log-loss):\n",
    "$$H_{y'}(y)=-\\sum_i y'_i\\log(y_i)$$\n",
    "Where $y$ is our predicted probability distribution, and $yâ€²$ is the true distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_ = tf.placeholder(tf.float32, [None, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The raw formulation of cross-entropy:  \n",
    "\n",
    "```cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))```  \n",
    "\n",
    "First, ```tf.log``` computes the logarithm of each element of $y$. Next, we multiply each element of $y\\_$ with the corresponding element of ```tf.log(y)```. Then ```tf.reduce_sum``` adds the elements in the second dimension of $y$, due to the ```reduction_indices=[1]``` parameter. Finally, ```tf.reduce_mean``` computes the mean over all the examples in the batch.\n",
    "Unfortunatelly, this can be numerically unstable.  \n",
    "Instead, we apply ```tf.nn.softmax_cross_entropy_with_logits``` on the unnormalized logits (e.g., we call ```softmax_cross_entropy_with_logits on tf.matmul(x, W) + b)```, because this more numerically stable function internally computes the softmax activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow knows the entire graph of your computations, it can automatically use the <a href=\"http://colah.github.io/posts/2015-08-Backprop/\">backpropagation algorithm</a> to efficiently determine how your variables affect the loss you ask it to minimize. Then it can apply your choice of optimization algorithm to modify the variables and reduce the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we ask TensorFlow to minimize ```cross_entropy``` using the **gradient descent algorithm** with a learning rate of 0.5. Gradient descent is a simple procedure, where TensorFlow simply shifts each variable a little bit in the direction that reduces the cost. But TensorFlow also provides many other <a href=\"https://www.tensorflow.org/api_guides/python/train#Optimizers\">optimization algorithms</a>: using one is as simple as tweaking one line. \n",
    "\n",
    "What TensorFlow actually does here, behind the scenes, is to add new operations to your graph which implement backpropagation and gradient descent. Then it gives you back a single operation which, when run, does a step of gradient descent training, slightly tweaking your variables to reduce the loss.\n",
    "\n",
    "Launch the model in an ```InteractiveSession```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.global_variables_initializer().run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the training step 1000 times!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for _ in range(1000):\n",
    "  batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "  sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each step of the loop, we get a \"batch\" of one hundred random data points from our training set. We run ```train_step``` feeding in the batches data to replace the ```placeholder```'s."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Model\n",
    "```tf.argmax``` gives you the index of the highest entry in a tensor along some axis. Use ```tf.equal``` to check if our prediction matches the truth:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That gives us a list of booleans. To determine what fraction are correct, we cast to floating point numbers and then take the mean. For example, ```[True, False, True, True]``` would become ```[1,0,1,1]``` which would become $0.75$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy on test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9075\n"
     ]
    }
   ],
   "source": [
    "print(sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels}))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
